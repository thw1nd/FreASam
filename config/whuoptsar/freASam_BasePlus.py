# freASam_Tiny.py  â€”â€” ç›´æ¥è¦†ç›–ä½¿ç”¨
from torch.utils.data import DataLoader
from geoseg.losses import *
from geoseg.datasets.whuoptsar_dataset import *
from geoseg.models.FreASam_for_2_modality.FreASam_BasePlus import FreASam
# from geoseg.models.FreASam.Image_Encoder import load_sam2_weights_to_multimodal_model
import os
from collections import OrderedDict

import torch
import torch.nn as nn
from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR


# =============== é€æ­¥è§£å†»çš„å¼€å…³ ===============
UNFREEZE_SCHEDULE = [
    # å…ˆåªåŠ¨æœ€å2ä¸ªblockçš„å½’ä¸€åŒ–ï¼›ç»™æ¨¡å‹â€œçƒ­èº«â€
    {"epoch": 10, "blocks": {"type": "last", "n": 6}, "parts": ["norm"]},

    {"epoch": 15, "blocks": {"type": "last", "n": 3}, "parts": ["mlp"]},

]

# =============== è¾…åŠ©ï¼šBN -> GNï¼ˆå° batch æ›´ç¨³ï¼‰ ===============
def _pick_group_count(C: int) -> int:
    for g in (32, 16, 8, 4, 2, 1):
        if C % g == 0:
            return g
    return 1

def bn_to_gn(module: nn.Module, eps: float = 1e-5, affine: bool = True):
    for name, child in list(module.named_children()):
        if isinstance(child, (nn.BatchNorm2d, nn.SyncBatchNorm)):
            C = child.num_features
            groups = _pick_group_count(C)
            setattr(module, name, nn.GroupNorm(groups, C, eps=eps, affine=affine))
        else:
            bn_to_gn(child, eps=eps, affine=affine)

# =============== åˆ†ç»„ä¸æ­£åˆ™åˆ¤æ–­ ===============
def _is_norm_like(name: str) -> bool:
    n = name.lower()
    if n.endswith(".bias"):
        return True
    if ("norm" in n) or ("bn" in n) or ("ln" in n) or ("gn" in n):
        return True
    return False

def _is_mlp_or_lora(name: str) -> bool:
    n = name.lower()
    if "mlp" in n:
        return True
    if "lora_a" in n or "lora_b" in n or "lora_down" in n or "lora_up" in n:
        return True
    return False

# =============== å…¼å®¹å¤šç§ ckpt çš„å– state_dict ===============
def _get_sam2_state_dict(ckpt_obj):
    if isinstance(ckpt_obj, dict):
        if 'model' in ckpt_obj and isinstance(ckpt_obj['model'], dict):
            return ckpt_obj['model']
        if 'state_dict' in ckpt_obj and isinstance(ckpt_obj['state_dict'], dict):
            return ckpt_obj['state_dict']
        if all(isinstance(v, torch.Tensor) for v in ckpt_obj.values()):
            return ckpt_obj
    raise RuntimeError("SAM2 ckpt ä¸å«å¯ç”¨çš„ state_dictï¼ˆæ—¢æ—  'model' ä¹Ÿæ—  'state_dict'ï¼‰ã€‚")

def _strip_trunk_prefix(name: str, trunk_prefix="image_encoder.trunk."):
    return name[len(trunk_prefix):] if name.startswith(trunk_prefix) else name

def _shape_compatible(t_src: torch.Tensor, t_dst: torch.Tensor) -> bool:
    return tuple(t_src.shape) == tuple(t_dst.shape)

# =============== æƒé‡åŠ è½½ + å†»ç»“ ===============
def load_sam2_weights_to_multimodal_model(
    model,
    sam2_ckpt_path: str,
    freeze_loaded: bool = True,
    verbose: bool = True,
    trunk_prefix: str = "image_encoder.trunk.",
    encoder_prefix: str = "encoder.",
    freeze_patterns_exclude = ("lora",),
):
    assert os.path.isfile(sam2_ckpt_path), f"æ‰¾ä¸åˆ°æƒé‡ï¼š{sam2_ckpt_path}"
    ckpt = torch.load(sam2_ckpt_path, map_location="cpu")
    print(f"==> è¯»å– SAM2 æƒé‡: {sam2_ckpt_path} | é¡¶å±‚é”®: {list(ckpt.keys())}")
    sam2_sd_raw = _get_sam2_state_dict(ckpt)

    # ä»…ä¿ç•™ trunk ç›¸å…³é”®ï¼›å¦‚æ— å‰ç¼€ï¼Œä¹Ÿå…¼å®¹ blocks./patch_embed./pos_embed*
    sam2_trunk = OrderedDict()
    for k, v in sam2_sd_raw.items():
        if k.startswith(trunk_prefix):
            sam2_trunk[_strip_trunk_prefix(k, trunk_prefix)] = v
        elif k.startswith(("blocks.", "patch_embed.", "pos_embed", "pos_embed_window", "norm", "x_norm")):
            sam2_trunk[k] = v

    model_sd = model.state_dict()
    loaded, skipped, shape_mismatch = [], [], []

    def _copy_if_ok(src_key, dst_key, src_t):
        if dst_key in model_sd:
            if _shape_compatible(src_t, model_sd[dst_key]):
                model_sd[dst_key].copy_(src_t)
                loaded.append(dst_key)
            else:
                shape_mismatch.append((src_key, dst_key, tuple(src_t.shape), tuple(model_sd[dst_key].shape)))
        else:
            # åªæœ‰åœ¨ç¡®å®æœŸæœ›å­˜åœ¨çš„ç›®æ ‡ä¸Šå†è®°å½• skippedï¼›é¿å…æŠŠæ— æ„ä¹‰çš„ aux1_* è®°æˆå™ªéŸ³
            pass

    # A) ä¸»åˆ†æ”¯ï¼šencoder.<subkey>   B) aux1ï¼šæŒ‰å®é™…å­˜åœ¨çš„ aux1_* æ¨¡å—ç²¾ç¡®æ˜ å°„ï¼Œä¸ç”¨é€šç”¨ aux1_ å‰ç¼€
    for src_key, src_tensor in sam2_trunk.items():
        subkey = src_key

        # 1) ä¸»å¹²ç›´æ¥å°è¯•
        dst_x = f"{encoder_prefix}{subkey}"
        _copy_if_ok(src_key, dst_x, src_tensor)

        # 2) patch_embed â†’ aux1_patch_embedï¼ˆè‹¥å­˜åœ¨ï¼‰
        if subkey.startswith("patch_embed."):
            dst_aux1 = f"{encoder_prefix}aux1_{subkey}"
            _copy_if_ok(src_key, dst_aux1, src_tensor)
            continue  # ç»§ç»­ä¸‹ä¸€ä¸ªé”®

        # 3) blocks.* å†…çš„ç»†åˆ†æ˜ å°„
        if subkey.startswith("blocks."):
            # 3.1 norm1 / norm2 â†’ aux1_norm1 / aux1_norm2
            if (".norm1" in subkey) or (".x_norm1" in subkey):
                aux1_subkey = subkey.replace(".norm1", ".aux1_norm1").replace(".x_norm1", ".aux1_norm1")
                _copy_if_ok(src_key, f"{encoder_prefix}{aux1_subkey}", src_tensor)

                # x_norm1 å…¼å®¹ï¼šå¦‚æœæ¨¡å‹é‡Œæœ‰ x_norm1ï¼Œå°±ä¹Ÿæ‹·ä¸€ä»½
                x_subkey_alt = subkey.replace(".norm1", ".x_norm1")
                _copy_if_ok(src_key, f"{encoder_prefix}{x_subkey_alt}", src_tensor)
                continue

            if (".norm2" in subkey) or (".x_norm2" in subkey):
                aux1_subkey = subkey.replace(".norm2", ".aux1_norm2").replace(".x_norm2", ".aux1_norm2")
                _copy_if_ok(src_key, f"{encoder_prefix}{aux1_subkey}", src_tensor)

                x_subkey_alt = subkey.replace(".norm2", ".x_norm2")
                _copy_if_ok(src_key, f"{encoder_prefix}{x_subkey_alt}", src_tensor)
                continue

            # 3.2 attn.proj.* â†’ åŒæ—¶å†™åˆ° aux1_proj.*
            if ".attn.proj." in subkey:
                _copy_if_ok(src_key, f"{encoder_prefix}{subkey}", src_tensor)  # x
                _copy_if_ok(src_key, f"{encoder_prefix}{subkey.replace('.attn.proj.', '.attn.aux1_proj.')}", src_tensor)  # aux1
                continue

            # 3.3 mlp.layers.* â†’ åŒæ—¶å†™åˆ° aux1_mlp.layers.*
            if ".mlp.layers." in subkey:
                _copy_if_ok(src_key, f"{encoder_prefix}{subkey}", src_tensor)  # x
                _copy_if_ok(src_key, f"{encoder_prefix}{subkey.replace('.mlp.', '.aux1_mlp.')}", src_tensor)  # aux1
                continue

            # 3.4 qkv / å…¶ä»–ï¼šä¸è¦å†ç”¨é€šç”¨ aux1_ å‰ç¼€å»çæ˜ å°„
            #    è‹¥ä½ çš„æ¨¡å‹ç¡®å®å­˜åœ¨å¯¹åº”çš„ aux1 è·¯å¾„ï¼ˆç½•è§ï¼‰ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ ç‰¹åˆ¤
            if ".attn.qkv." in subkey:
                # ä»…ä¸»å¹²ï¼šå·²ç»åœ¨ 1) é‡Œå°è¯•è¿‡ï¼Œæ— éœ€é¢å¤– aux1*
                continue

            # å…¶ä½™ blocks.* çš„é”®ï¼Œ1) å·²å°è¯•ä¸»å¹²å¤åˆ¶ï¼›aux1 ä¸å†é€šé…

        # 4) é¡¶å±‚ norm / pos_embed / pos_embed_windowï¼Œå¦‚æ¨¡å‹å­˜åœ¨ aux1_* æ‰å¤åˆ¶
        if subkey.startswith(("norm", "x_norm", "pos_embed", "pos_embed_window")):
            _copy_if_ok(src_key, f"{encoder_prefix}aux1_{subkey}", src_tensor)

    # è¦†ç›– state_dict
    model.load_state_dict(model_sd, strict=False)

    # å¯é€‰ï¼šå†»ç»“å·²åŠ è½½å‚æ•°ï¼ˆæ’é™¤ LoRA ç­‰å…³é”®è¯ï¼‰
    if freeze_loaded:
        frozen, total = 0, 0
        for n, p in model.named_parameters():
            total += 1
            if any(x in n for x in freeze_patterns_exclude):
                continue
            if n in loaded:
                p.requires_grad_(False)
                frozen += 1
        if verbose:
            print(f"[SAM2->Encoder] å·²åŠ è½½(å¹¶å¯èƒ½å†»ç»“) {len(loaded)} å±‚ / å†»ç»“ {frozen}/{total}")

    if verbose:
        print(f"[SAM2->Encoder] å·²åŠ è½½(å¹¶å¯èƒ½å†»ç»“) {len(loaded)} å±‚")
        if shape_mismatch:
            print(f"  ! å½¢çŠ¶ä¸åŒ¹é… {len(shape_mismatch)} ä¾‹ï¼ˆå‰10æ¡ï¼‰ï¼š")
            for i, (sk, dk, ss, ds) in enumerate(shape_mismatch[:10]):
                print(f"    - {sk} -> {dk}, src{ss} vs dst{ds}")
        if skipped:
            print(f"  â€¦ è·³è¿‡ï¼ˆç›®æ ‡ä¸å­˜åœ¨ï¼‰{len(skipped)} ä¾‹ï¼ˆå‰10æ¡ï¼‰ï¼š")
            for i, (sk, dk) in enumerate(skipped[:10]):
                print(f"    - {sk} -> {dk}")

    return {"loaded": loaded, "shape_mismatch": shape_mismatch, "skipped": skipped}

# ----------------------------- è®­ç»ƒè¶…å‚ -----------------------------
max_epoch = 80
warmup_epochs = 8
eta_min = 5e-5
ignore_index = 255
background_index = None
train_batch_size = 8
val_batch_size = 8

# åˆ†ç»„å­¦ä¹ ç‡
lr = 2.25e-3            # å¤´éƒ¨
backbone_lr = 2.25e-4   # encoder ä¸»å¹²
lr_enc_mlp = 3.0e-4     # encoder çš„ MLP/LoRA

weight_decay = 1e-2
backbone_weight_decay = 1e-2

# ç±»åˆ«æƒé‡å¹³æ»‘
# base_class_weights = torch.tensor([1, 1, 1, 1, 1, 3, 4], dtype=torch.float32)
# ramp_epochs_for_class_weights = 15

accumulate_n = 4
num_classes = 7
classes = CLASSES

test_time_aug = 'lr'
output_mask_dir, output_mask_rgb_dir = None, None
weights_name = "FreASam-BasePlus"
weights_path = f"model_weights/whuoptsar/two-modality/{weights_name}"
metrics_txt_path = f"model_weights/whuoptsar/two-modality/text/{weights_name}"
test_weights_name = "FreASam-BasePlus"
log_name = f'potsdam/{weights_name}'
monitor = 'val_F1'
monitor_mode = 'max'
save_top_k = 1
save_last = True
check_val_every_n_epoch = 1
gpus = [0]
strategy = 'auto'
pretrained_ckpt_path = None
resume_ckpt_path = None

# ----------------------------- æ„å»ºæ¨¡å‹ + è¯» SAM2 + å†»ç»“ -----------------------------
net = FreASam(n_class=7)
sam2_weights_path = r"D:\\ESUWork\\sam2.1_hiera_base_plus.pt"
summary = load_sam2_weights_to_multimodal_model(
    model=net,
    sam2_ckpt_path=sam2_weights_path,
    freeze_loaded=True,   # åŠ è½½åå…ˆå†»ç»“ï¼ˆLoRA ä¸å†»ç»“ï¼‰
    verbose=True
)

# BN->GN
bn_to_gn(net)

# æ‰“å°å†»ç»“å±‚
frozen_params = [n for n, p in net.named_parameters() if not p.requires_grad]
print("å†»ç»“çš„æƒé‡å±‚ï¼š")
for name in frozen_params:
    print(name)

# ----------------------------- Loss -----------------------------
loss = UnetFormerLoss(ignore_index=ignore_index)
use_aux_loss = True

# ----------------------------- æ•°æ® -----------------------------
train_dataset = WhuoptsarDataset(
    data_root=r'H:\Datasets\WHUOPTSAR\data_512\data\aug_train',
    mosaic_ratio=0.0,
    transform=train_aug
)
val_dataset = WhuoptsarDataset(
    data_root=r'H:\Datasets\WHUOPTSAR\data_512\data\test',
    mosaic_ratio=0.0,
    transform=val_aug
)

train_loader = DataLoader(train_dataset, batch_size=train_batch_size,
                          num_workers=4, pin_memory=True, shuffle=True,
                          drop_last=True, persistent_workers=True)
val_loader = DataLoader(val_dataset, batch_size=val_batch_size,
                        num_workers=4, shuffle=False, pin_memory=True,
                        drop_last=False, persistent_workers=True)

# ----------------------------- ä¼˜åŒ–å™¨åˆ†ç»„ -----------------------------
loaded_set = set(summary.get("loaded", []))

def _is_sam2_loaded_encoder_param(name_in_encoder: str) -> bool:
    """
    name_in_encoder å½¢å¦‚ 'blocks.0.attn.qkv.weight'
    summary["loaded"] é‡Œå­˜çš„æ˜¯ 'encoder.blocks.0.attn.qkv.weight'
    """
    full_name = f"encoder.{name_in_encoder}"
    return full_name in loaded_set

enc_backbone_decay, enc_backbone_nodecay = [], []   # SAM2 ä¸»å¹²ä¸Šè¢«è§£å†»çš„é‚£äº›å‚æ•°ï¼ˆå° LRï¼‰
enc_mlp_decay, enc_mlp_nodecay = [], []             # encoder é‡Œçš„ MLP / LoRAï¼ˆä¸­ LRï¼‰
enc_new_decay, enc_new_nodecay = [], []             # encoder é‡Œâ€œæ–°åŠ çš„æ¨¡å—â€ï¼ˆé SAM2 & é LoRA/MLPï¼‰ï¼Œç”¨å¤§ LR
head_decay, head_nodecay = [], []                   # encoder ä¹‹å¤–çš„æ‰€æœ‰æ¨¡å—ï¼ˆdecoder, seg-head, fusion ç­‰ï¼‰

# å…ˆå¤„ç† encoder å†…éƒ¨
for n, p in net.encoder.named_parameters():
    # å†»ç»“çš„å‚æ•°å…ˆä¸ç®¡ requires_gradï¼Œä»ç„¶æ”¾è¿› group é‡Œï¼Œåé¢è§£å†»æ—¶å¯ä»¥ç›´æ¥ç”¨
    is_mlp_lora = _is_mlp_or_lora(n)
    no_decay = _is_norm_like(n) or is_mlp_lora
    is_loaded_backbone = _is_sam2_loaded_encoder_param(n)

    if is_mlp_lora:
        # æ‰€æœ‰ LoRA / MLP ç»Ÿä¸€æ”¾åœ¨ enc_mlp_* ç»„ï¼Œç”¨ lr_enc_mlp
        (enc_mlp_nodecay if no_decay else enc_mlp_decay).append(p)
    else:
        # é LoRA / MLPï¼šå†åŒºåˆ†ã€Œæ¥è‡ª SAM2ã€è¿˜æ˜¯ã€Œä½ æ–°å†™çš„æ¨¡å—ã€
        if is_loaded_backbone:
            # è¿™æ˜¯ SAM2 ä¸»å¹²ä¸Šè¢«è§£å†»çš„é‚£éƒ¨åˆ†ï¼ˆä¾‹å¦‚æŒ‰ schedule è§£å†»çš„ norm / æœ€å 2 ä¸ª block çš„ qkv/projï¼‰
            (enc_backbone_nodecay if no_decay else enc_backbone_decay).append(p)
        else:
            # è¿™æ˜¯ä½ åœ¨ encoder é‡Œæ–°å†™çš„ç»“æ„ï¼ˆä¾‹å¦‚ CrossModalFuseModule çš„å·ç§¯ã€cross_gamma ç­‰ï¼‰
            # è®­ç»ƒç­–ç•¥ä¸Šæ›´åƒâ€œå¤´éƒ¨â€ï¼Œç”¨è¾ƒå¤§çš„ LR æ›´åˆé€‚
            (enc_new_nodecay if no_decay else enc_new_decay).append(p)

# å†å¤„ç† encoder ä¹‹å¤–çš„æ‰€æœ‰å‚æ•°ï¼šç»Ÿä¸€å½“ä½œ headï¼ˆdecoder / seg-head / fusion ç­‰ï¼‰
for n, p in net.named_parameters():
    if n.startswith("encoder."):
        continue
    (head_nodecay if _is_norm_like(n) else head_decay).append(p)

optimizer_groups = [
    # 1) SAM2 ä¸»å¹²ä¸Šè¢«è§£å†»çš„é‚£ä¸€å°æ’®å‚æ•°ï¼ˆä¾‹å¦‚ä½ æŒ‰ UNFREEZE_SCHEDULE æ”¾å¼€çš„é‚£å‡ å±‚ï¼‰
    {"params": enc_backbone_decay,    "lr": backbone_lr, "weight_decay": backbone_weight_decay, "name": "enc_backbone_decay"},
    {"params": enc_backbone_nodecay,  "lr": backbone_lr, "weight_decay": 0.0,                   "name": "enc_backbone_nodecay"},

    # 2) encoder é‡Œçš„ MLP / LoRAï¼ˆåŒ…æ‹¬ qkv é‡Œçš„ LoRAã€CrossModalFuse é‡Œå¸¦ lora_xxx çš„çº¿æ€§å±‚ï¼‰
    {"params": enc_mlp_decay,         "lr": lr_enc_mlp,  "weight_decay": backbone_weight_decay, "name": "enc_mlp_decay"},
    {"params": enc_mlp_nodecay,       "lr": lr_enc_mlp,  "weight_decay": 0.0,                   "name": "enc_mlp_nodecay"},

    # 3) ä½ åœ¨ encoder é‡Œæ–°å†™çš„æ¨¡å—ï¼ˆé SAM2 & é LoRA/MLPï¼‰ï¼Œç”¨å’Œ head ä¸€æ ·çš„ LR
    {"params": enc_new_decay,         "lr": lr,          "weight_decay": weight_decay,          "name": "enc_new_decay"},
    {"params": enc_new_nodecay,       "lr": lr,          "weight_decay": 0.0,                   "name": "enc_new_nodecay"},

    # 4) encoder ä¹‹å¤–çš„æ‰€æœ‰å±‚ï¼ˆdecoder / seg-head / fusion ç­‰ï¼‰ï¼Œä¼ ç»Ÿæ„ä¹‰ä¸Šçš„â€œå¤´éƒ¨â€
    {"params": head_decay,            "lr": lr,          "weight_decay": weight_decay,          "name": "head_decay"},
    {"params": head_nodecay,          "lr": lr,          "weight_decay": 0.0,                   "name": "head_nodecay"},
]

# å»æ‰ç©ºçš„ param groupï¼Œé¿å…è­¦å‘Š
optimizer_groups = [g for g in optimizer_groups if len(g["params"]) > 0]

print("ğŸ“Š Optimizer groups:")
for g in optimizer_groups:
    print(f"  - {g['name']}: {len(g['params'])} params, lr={g['lr']}, wd={g['weight_decay']}")

optimizer = torch.optim.AdamW(optimizer_groups, betas=(0.9, 0.99), eps=1e-6, amsgrad=False)

# ----------------------------- LR Schedulers -----------------------------
def _warmup_lambda(cur_epoch: int):
    return (cur_epoch + 1) / max(1, warmup_epochs)

warmup_scheduler  = LambdaLR(optimizer, lr_lambda=_warmup_lambda)
cosine_scheduler  = CosineAnnealingLR(optimizer, T_max=max(1, max_epoch - warmup_epochs), eta_min=eta_min)
lr_scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])
